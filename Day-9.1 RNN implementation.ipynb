{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c459fd2c",
   "metadata": {},
   "source": [
    "# This is a basic mathematical interpretation of a simple RNN. In practice, more sophisticated variants like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are often used to address issues like vanishing gradients and better capture long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af790e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d6b88",
   "metadata": {},
   "source": [
    "Dense:- This represents a **fully connected layer** (also known as a \"dense\" layer or \"feed-forward\" layer). In a Dense layer, every neuron in the layer is connected to every neuron in the preceding layer. These layers are commonly **used** in various neural network architectures **for learning complex patterns and relationships in the data**, often serving as output layers or intermediate layers in deep learning models.\n",
    "\n",
    "SimpleRNN:- SimpleRNN layers are designed to process sequential data, where the output at a given time step depends not only on the current input but also on the previous hidden state. They are a fundamental building block for tasks involving time series, natural language processing, and other sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf55a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 20:30:24.651957: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-23 20:30:27.048169: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-23 20:30:27.942468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755961228.711025    4658 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755961228.883651    4658 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755961230.375040    4658 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755961230.375172    4658 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755961230.375186    4658 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755961230.375199    4658 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-23 20:30:30.543105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential #It provides a straightforward way to create and organize the layers \n",
    "                                                # of a deep learning model in a sequential fashion. \n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99de527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "X = np.array([[i+j for j in range(5)] for i in range(100)])\n",
    "y = np.array([i+5 for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cee16be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,\n",
       "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,\n",
       "        31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
       "        44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "        57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "        83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
       "        96,  97,  98,  99, 100, 101, 102, 103, 104])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736dbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for RNN input (samples, time steps, features)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1)) #It keeps the original number of rows and columns, but adds a \n",
    "                                           # new dimension of size 1 at the end.\n",
    "#This is often done to prepare data for certain machine learning models that expect a specific number of \n",
    "#dimensions, such as convolutional neural networks that often process image data with channels as the last dimension.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b774e0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0],\n",
       "        [  1],\n",
       "        [  2],\n",
       "        [  3],\n",
       "        [  4]],\n",
       "\n",
       "       [[  1],\n",
       "        [  2],\n",
       "        [  3],\n",
       "        [  4],\n",
       "        [  5]],\n",
       "\n",
       "       [[  2],\n",
       "        [  3],\n",
       "        [  4],\n",
       "        [  5],\n",
       "        [  6]],\n",
       "\n",
       "       [[  3],\n",
       "        [  4],\n",
       "        [  5],\n",
       "        [  6],\n",
       "        [  7]],\n",
       "\n",
       "       [[  4],\n",
       "        [  5],\n",
       "        [  6],\n",
       "        [  7],\n",
       "        [  8]],\n",
       "\n",
       "       [[  5],\n",
       "        [  6],\n",
       "        [  7],\n",
       "        [  8],\n",
       "        [  9]],\n",
       "\n",
       "       [[  6],\n",
       "        [  7],\n",
       "        [  8],\n",
       "        [  9],\n",
       "        [ 10]],\n",
       "\n",
       "       [[  7],\n",
       "        [  8],\n",
       "        [  9],\n",
       "        [ 10],\n",
       "        [ 11]],\n",
       "\n",
       "       [[  8],\n",
       "        [  9],\n",
       "        [ 10],\n",
       "        [ 11],\n",
       "        [ 12]],\n",
       "\n",
       "       [[  9],\n",
       "        [ 10],\n",
       "        [ 11],\n",
       "        [ 12],\n",
       "        [ 13]],\n",
       "\n",
       "       [[ 10],\n",
       "        [ 11],\n",
       "        [ 12],\n",
       "        [ 13],\n",
       "        [ 14]],\n",
       "\n",
       "       [[ 11],\n",
       "        [ 12],\n",
       "        [ 13],\n",
       "        [ 14],\n",
       "        [ 15]],\n",
       "\n",
       "       [[ 12],\n",
       "        [ 13],\n",
       "        [ 14],\n",
       "        [ 15],\n",
       "        [ 16]],\n",
       "\n",
       "       [[ 13],\n",
       "        [ 14],\n",
       "        [ 15],\n",
       "        [ 16],\n",
       "        [ 17]],\n",
       "\n",
       "       [[ 14],\n",
       "        [ 15],\n",
       "        [ 16],\n",
       "        [ 17],\n",
       "        [ 18]],\n",
       "\n",
       "       [[ 15],\n",
       "        [ 16],\n",
       "        [ 17],\n",
       "        [ 18],\n",
       "        [ 19]],\n",
       "\n",
       "       [[ 16],\n",
       "        [ 17],\n",
       "        [ 18],\n",
       "        [ 19],\n",
       "        [ 20]],\n",
       "\n",
       "       [[ 17],\n",
       "        [ 18],\n",
       "        [ 19],\n",
       "        [ 20],\n",
       "        [ 21]],\n",
       "\n",
       "       [[ 18],\n",
       "        [ 19],\n",
       "        [ 20],\n",
       "        [ 21],\n",
       "        [ 22]],\n",
       "\n",
       "       [[ 19],\n",
       "        [ 20],\n",
       "        [ 21],\n",
       "        [ 22],\n",
       "        [ 23]],\n",
       "\n",
       "       [[ 20],\n",
       "        [ 21],\n",
       "        [ 22],\n",
       "        [ 23],\n",
       "        [ 24]],\n",
       "\n",
       "       [[ 21],\n",
       "        [ 22],\n",
       "        [ 23],\n",
       "        [ 24],\n",
       "        [ 25]],\n",
       "\n",
       "       [[ 22],\n",
       "        [ 23],\n",
       "        [ 24],\n",
       "        [ 25],\n",
       "        [ 26]],\n",
       "\n",
       "       [[ 23],\n",
       "        [ 24],\n",
       "        [ 25],\n",
       "        [ 26],\n",
       "        [ 27]],\n",
       "\n",
       "       [[ 24],\n",
       "        [ 25],\n",
       "        [ 26],\n",
       "        [ 27],\n",
       "        [ 28]],\n",
       "\n",
       "       [[ 25],\n",
       "        [ 26],\n",
       "        [ 27],\n",
       "        [ 28],\n",
       "        [ 29]],\n",
       "\n",
       "       [[ 26],\n",
       "        [ 27],\n",
       "        [ 28],\n",
       "        [ 29],\n",
       "        [ 30]],\n",
       "\n",
       "       [[ 27],\n",
       "        [ 28],\n",
       "        [ 29],\n",
       "        [ 30],\n",
       "        [ 31]],\n",
       "\n",
       "       [[ 28],\n",
       "        [ 29],\n",
       "        [ 30],\n",
       "        [ 31],\n",
       "        [ 32]],\n",
       "\n",
       "       [[ 29],\n",
       "        [ 30],\n",
       "        [ 31],\n",
       "        [ 32],\n",
       "        [ 33]],\n",
       "\n",
       "       [[ 30],\n",
       "        [ 31],\n",
       "        [ 32],\n",
       "        [ 33],\n",
       "        [ 34]],\n",
       "\n",
       "       [[ 31],\n",
       "        [ 32],\n",
       "        [ 33],\n",
       "        [ 34],\n",
       "        [ 35]],\n",
       "\n",
       "       [[ 32],\n",
       "        [ 33],\n",
       "        [ 34],\n",
       "        [ 35],\n",
       "        [ 36]],\n",
       "\n",
       "       [[ 33],\n",
       "        [ 34],\n",
       "        [ 35],\n",
       "        [ 36],\n",
       "        [ 37]],\n",
       "\n",
       "       [[ 34],\n",
       "        [ 35],\n",
       "        [ 36],\n",
       "        [ 37],\n",
       "        [ 38]],\n",
       "\n",
       "       [[ 35],\n",
       "        [ 36],\n",
       "        [ 37],\n",
       "        [ 38],\n",
       "        [ 39]],\n",
       "\n",
       "       [[ 36],\n",
       "        [ 37],\n",
       "        [ 38],\n",
       "        [ 39],\n",
       "        [ 40]],\n",
       "\n",
       "       [[ 37],\n",
       "        [ 38],\n",
       "        [ 39],\n",
       "        [ 40],\n",
       "        [ 41]],\n",
       "\n",
       "       [[ 38],\n",
       "        [ 39],\n",
       "        [ 40],\n",
       "        [ 41],\n",
       "        [ 42]],\n",
       "\n",
       "       [[ 39],\n",
       "        [ 40],\n",
       "        [ 41],\n",
       "        [ 42],\n",
       "        [ 43]],\n",
       "\n",
       "       [[ 40],\n",
       "        [ 41],\n",
       "        [ 42],\n",
       "        [ 43],\n",
       "        [ 44]],\n",
       "\n",
       "       [[ 41],\n",
       "        [ 42],\n",
       "        [ 43],\n",
       "        [ 44],\n",
       "        [ 45]],\n",
       "\n",
       "       [[ 42],\n",
       "        [ 43],\n",
       "        [ 44],\n",
       "        [ 45],\n",
       "        [ 46]],\n",
       "\n",
       "       [[ 43],\n",
       "        [ 44],\n",
       "        [ 45],\n",
       "        [ 46],\n",
       "        [ 47]],\n",
       "\n",
       "       [[ 44],\n",
       "        [ 45],\n",
       "        [ 46],\n",
       "        [ 47],\n",
       "        [ 48]],\n",
       "\n",
       "       [[ 45],\n",
       "        [ 46],\n",
       "        [ 47],\n",
       "        [ 48],\n",
       "        [ 49]],\n",
       "\n",
       "       [[ 46],\n",
       "        [ 47],\n",
       "        [ 48],\n",
       "        [ 49],\n",
       "        [ 50]],\n",
       "\n",
       "       [[ 47],\n",
       "        [ 48],\n",
       "        [ 49],\n",
       "        [ 50],\n",
       "        [ 51]],\n",
       "\n",
       "       [[ 48],\n",
       "        [ 49],\n",
       "        [ 50],\n",
       "        [ 51],\n",
       "        [ 52]],\n",
       "\n",
       "       [[ 49],\n",
       "        [ 50],\n",
       "        [ 51],\n",
       "        [ 52],\n",
       "        [ 53]],\n",
       "\n",
       "       [[ 50],\n",
       "        [ 51],\n",
       "        [ 52],\n",
       "        [ 53],\n",
       "        [ 54]],\n",
       "\n",
       "       [[ 51],\n",
       "        [ 52],\n",
       "        [ 53],\n",
       "        [ 54],\n",
       "        [ 55]],\n",
       "\n",
       "       [[ 52],\n",
       "        [ 53],\n",
       "        [ 54],\n",
       "        [ 55],\n",
       "        [ 56]],\n",
       "\n",
       "       [[ 53],\n",
       "        [ 54],\n",
       "        [ 55],\n",
       "        [ 56],\n",
       "        [ 57]],\n",
       "\n",
       "       [[ 54],\n",
       "        [ 55],\n",
       "        [ 56],\n",
       "        [ 57],\n",
       "        [ 58]],\n",
       "\n",
       "       [[ 55],\n",
       "        [ 56],\n",
       "        [ 57],\n",
       "        [ 58],\n",
       "        [ 59]],\n",
       "\n",
       "       [[ 56],\n",
       "        [ 57],\n",
       "        [ 58],\n",
       "        [ 59],\n",
       "        [ 60]],\n",
       "\n",
       "       [[ 57],\n",
       "        [ 58],\n",
       "        [ 59],\n",
       "        [ 60],\n",
       "        [ 61]],\n",
       "\n",
       "       [[ 58],\n",
       "        [ 59],\n",
       "        [ 60],\n",
       "        [ 61],\n",
       "        [ 62]],\n",
       "\n",
       "       [[ 59],\n",
       "        [ 60],\n",
       "        [ 61],\n",
       "        [ 62],\n",
       "        [ 63]],\n",
       "\n",
       "       [[ 60],\n",
       "        [ 61],\n",
       "        [ 62],\n",
       "        [ 63],\n",
       "        [ 64]],\n",
       "\n",
       "       [[ 61],\n",
       "        [ 62],\n",
       "        [ 63],\n",
       "        [ 64],\n",
       "        [ 65]],\n",
       "\n",
       "       [[ 62],\n",
       "        [ 63],\n",
       "        [ 64],\n",
       "        [ 65],\n",
       "        [ 66]],\n",
       "\n",
       "       [[ 63],\n",
       "        [ 64],\n",
       "        [ 65],\n",
       "        [ 66],\n",
       "        [ 67]],\n",
       "\n",
       "       [[ 64],\n",
       "        [ 65],\n",
       "        [ 66],\n",
       "        [ 67],\n",
       "        [ 68]],\n",
       "\n",
       "       [[ 65],\n",
       "        [ 66],\n",
       "        [ 67],\n",
       "        [ 68],\n",
       "        [ 69]],\n",
       "\n",
       "       [[ 66],\n",
       "        [ 67],\n",
       "        [ 68],\n",
       "        [ 69],\n",
       "        [ 70]],\n",
       "\n",
       "       [[ 67],\n",
       "        [ 68],\n",
       "        [ 69],\n",
       "        [ 70],\n",
       "        [ 71]],\n",
       "\n",
       "       [[ 68],\n",
       "        [ 69],\n",
       "        [ 70],\n",
       "        [ 71],\n",
       "        [ 72]],\n",
       "\n",
       "       [[ 69],\n",
       "        [ 70],\n",
       "        [ 71],\n",
       "        [ 72],\n",
       "        [ 73]],\n",
       "\n",
       "       [[ 70],\n",
       "        [ 71],\n",
       "        [ 72],\n",
       "        [ 73],\n",
       "        [ 74]],\n",
       "\n",
       "       [[ 71],\n",
       "        [ 72],\n",
       "        [ 73],\n",
       "        [ 74],\n",
       "        [ 75]],\n",
       "\n",
       "       [[ 72],\n",
       "        [ 73],\n",
       "        [ 74],\n",
       "        [ 75],\n",
       "        [ 76]],\n",
       "\n",
       "       [[ 73],\n",
       "        [ 74],\n",
       "        [ 75],\n",
       "        [ 76],\n",
       "        [ 77]],\n",
       "\n",
       "       [[ 74],\n",
       "        [ 75],\n",
       "        [ 76],\n",
       "        [ 77],\n",
       "        [ 78]],\n",
       "\n",
       "       [[ 75],\n",
       "        [ 76],\n",
       "        [ 77],\n",
       "        [ 78],\n",
       "        [ 79]],\n",
       "\n",
       "       [[ 76],\n",
       "        [ 77],\n",
       "        [ 78],\n",
       "        [ 79],\n",
       "        [ 80]],\n",
       "\n",
       "       [[ 77],\n",
       "        [ 78],\n",
       "        [ 79],\n",
       "        [ 80],\n",
       "        [ 81]],\n",
       "\n",
       "       [[ 78],\n",
       "        [ 79],\n",
       "        [ 80],\n",
       "        [ 81],\n",
       "        [ 82]],\n",
       "\n",
       "       [[ 79],\n",
       "        [ 80],\n",
       "        [ 81],\n",
       "        [ 82],\n",
       "        [ 83]],\n",
       "\n",
       "       [[ 80],\n",
       "        [ 81],\n",
       "        [ 82],\n",
       "        [ 83],\n",
       "        [ 84]],\n",
       "\n",
       "       [[ 81],\n",
       "        [ 82],\n",
       "        [ 83],\n",
       "        [ 84],\n",
       "        [ 85]],\n",
       "\n",
       "       [[ 82],\n",
       "        [ 83],\n",
       "        [ 84],\n",
       "        [ 85],\n",
       "        [ 86]],\n",
       "\n",
       "       [[ 83],\n",
       "        [ 84],\n",
       "        [ 85],\n",
       "        [ 86],\n",
       "        [ 87]],\n",
       "\n",
       "       [[ 84],\n",
       "        [ 85],\n",
       "        [ 86],\n",
       "        [ 87],\n",
       "        [ 88]],\n",
       "\n",
       "       [[ 85],\n",
       "        [ 86],\n",
       "        [ 87],\n",
       "        [ 88],\n",
       "        [ 89]],\n",
       "\n",
       "       [[ 86],\n",
       "        [ 87],\n",
       "        [ 88],\n",
       "        [ 89],\n",
       "        [ 90]],\n",
       "\n",
       "       [[ 87],\n",
       "        [ 88],\n",
       "        [ 89],\n",
       "        [ 90],\n",
       "        [ 91]],\n",
       "\n",
       "       [[ 88],\n",
       "        [ 89],\n",
       "        [ 90],\n",
       "        [ 91],\n",
       "        [ 92]],\n",
       "\n",
       "       [[ 89],\n",
       "        [ 90],\n",
       "        [ 91],\n",
       "        [ 92],\n",
       "        [ 93]],\n",
       "\n",
       "       [[ 90],\n",
       "        [ 91],\n",
       "        [ 92],\n",
       "        [ 93],\n",
       "        [ 94]],\n",
       "\n",
       "       [[ 91],\n",
       "        [ 92],\n",
       "        [ 93],\n",
       "        [ 94],\n",
       "        [ 95]],\n",
       "\n",
       "       [[ 92],\n",
       "        [ 93],\n",
       "        [ 94],\n",
       "        [ 95],\n",
       "        [ 96]],\n",
       "\n",
       "       [[ 93],\n",
       "        [ 94],\n",
       "        [ 95],\n",
       "        [ 96],\n",
       "        [ 97]],\n",
       "\n",
       "       [[ 94],\n",
       "        [ 95],\n",
       "        [ 96],\n",
       "        [ 97],\n",
       "        [ 98]],\n",
       "\n",
       "       [[ 95],\n",
       "        [ 96],\n",
       "        [ 97],\n",
       "        [ 98],\n",
       "        [ 99]],\n",
       "\n",
       "       [[ 96],\n",
       "        [ 97],\n",
       "        [ 98],\n",
       "        [ 99],\n",
       "        [100]],\n",
       "\n",
       "       [[ 97],\n",
       "        [ 98],\n",
       "        [ 99],\n",
       "        [100],\n",
       "        [101]],\n",
       "\n",
       "       [[ 98],\n",
       "        [ 99],\n",
       "        [100],\n",
       "        [101],\n",
       "        [102]],\n",
       "\n",
       "       [[ 99],\n",
       "        [100],\n",
       "        [101],\n",
       "        [102],\n",
       "        [103]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe886dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 20:31:00.865492: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/kashika/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "#units=32 is the number of hidden units in the SimpleRNN layer.\n",
    "#The full input tensor for a SimpleRNN layer should have the shape (batch_size, timesteps, input_features)\n",
    "#X.shape[1] represents the number of timesteps in each sequence.\n",
    "#X.shape[2] represents the number of features per timestep. \n",
    "Num_model = Sequential([\n",
    "    SimpleRNN(units=32, input_shape=(X.shape[1], X.shape[2]), activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ace7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 562.1304\n",
      "Epoch 2/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 171.1600\n",
      "Epoch 3/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 17.8351\n",
      "Epoch 4/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 20.7057\n",
      "Epoch 5/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 49.4385\n",
      "Epoch 6/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 28.7266\n",
      "Epoch 7/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4138\n",
      "Epoch 8/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.2337\n",
      "Epoch 9/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 11.7771\n",
      "Epoch 10/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.1611\n",
      "Epoch 11/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.6754\n",
      "Epoch 12/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.3346\n",
      "Epoch 13/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.6017\n",
      "Epoch 14/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.1270\n",
      "Epoch 15/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.8112\n",
      "Epoch 16/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.4380\n",
      "Epoch 17/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8479\n",
      "Epoch 18/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.0983\n",
      "Epoch 19/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0327\n",
      "Epoch 20/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8634\n",
      "Epoch 21/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.9861\n",
      "Epoch 22/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.1268\n",
      "Epoch 23/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.9570\n",
      "Epoch 24/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.8698\n",
      "Epoch 25/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.0791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f383c4f7ee0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "Num_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "Num_model.fit(X, y, epochs=25, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80129216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[100],\n",
       "        [101],\n",
       "        [102],\n",
       "        [103],\n",
       "        [104]],\n",
       "\n",
       "       [[101],\n",
       "        [102],\n",
       "        [103],\n",
       "        [104],\n",
       "        [105]],\n",
       "\n",
       "       [[102],\n",
       "        [103],\n",
       "        [104],\n",
       "        [105],\n",
       "        [106]],\n",
       "\n",
       "       [[103],\n",
       "        [104],\n",
       "        [105],\n",
       "        [106],\n",
       "        [107]],\n",
       "\n",
       "       [[104],\n",
       "        [105],\n",
       "        [106],\n",
       "        [107],\n",
       "        [108]],\n",
       "\n",
       "       [[105],\n",
       "        [106],\n",
       "        [107],\n",
       "        [108],\n",
       "        [109]],\n",
       "\n",
       "       [[106],\n",
       "        [107],\n",
       "        [108],\n",
       "        [109],\n",
       "        [110]],\n",
       "\n",
       "       [[107],\n",
       "        [108],\n",
       "        [109],\n",
       "        [110],\n",
       "        [111]],\n",
       "\n",
       "       [[108],\n",
       "        [109],\n",
       "        [110],\n",
       "        [111],\n",
       "        [112]],\n",
       "\n",
       "       [[109],\n",
       "        [110],\n",
       "        [111],\n",
       "        [112],\n",
       "        [113]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "test_input = np.array([[i+j for j in range(5)] for i in range(100, 110)])\n",
    "test_input = test_input.reshape((test_input.shape[0], test_input.shape[1], 1))\n",
    "predicted_output = Num_model.predict(test_input)\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c677cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output:\n",
      "[106.439545 107.480965 108.52236  109.56377  110.60516  111.64658\n",
      " 112.68799  113.7294   114.7708   115.812195]\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted output\n",
    "print(\"Predicted Output:\")\n",
    "print(predicted_output.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025f90b",
   "metadata": {},
   "source": [
    "# Let's create a simple RNN using Keras with some sample data. In this example, we'll use a sequence of numbers to predict the next number in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2024aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 26.7911\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26.3617 \n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.9372 \n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0605 \n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.5619 \n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0023\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4788\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2776 \n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2622\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2508\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "Predicted Output: 4.695024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "sequence_length = 10\n",
    "X = np.random.rand(100, sequence_length)\n",
    "y = np.sum(X, axis=1)\n",
    "\n",
    "# Reshape the data for RNN input (samples, time steps, features)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(units=32, input_shape=(X.shape[1], X.shape[2]), activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=8)\n",
    "\n",
    "# Test the model\n",
    "test_input = np.random.rand(10).reshape((1, sequence_length, 1))\n",
    "predicted_output = model.predict(test_input)\n",
    "\n",
    "# Print the predicted output\n",
    "print(\"Predicted Output:\", predicted_output[0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
